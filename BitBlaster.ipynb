{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL dependencies\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "from gym import Env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common import env_checker\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory reading dependencies\n",
    "import pymem\n",
    "from pymem.process import module_from_name\n",
    "from pymem.ptypes import RemotePointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL.Image import fromarray\n",
    "import mss\n",
    "import pydirectinput\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read game variables from memory - https://stackoverflow.com/a/73538848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access game process\n",
    "pm = pymem.Pymem(\"BitBlasterXL.exe\")\n",
    "gameModule = module_from_name(pm.process_handle, \"UnityPlayer.dll\").lpBaseOfDll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthModule = int(gameModule+0x01ACA468)\n",
    "scoreModule = int(gameModule+0x01993010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom gym Env\n",
    "class BitBlaster(Env):\n",
    "    def __init__(self):\n",
    "        self.observation_space = Box(0, 255, shape=(1, 80, 80), dtype=np.uint8)\n",
    "        # low number of action_space actions for testing\n",
    "        self.action_space = Discrete(2)\n",
    "        # the game is in windowed mode, and placed on the top left of my screen to be captured\n",
    "        self.monitor = {\"top\": 35, \"left\": 0, \"width\": 644, \"height\": 484}\n",
    "        self.actionsDict = {0:\"right\", 1:\"ctrl\"}\n",
    "        self.previousScore = 0\n",
    "        self.previousHealth = 28\n",
    "\n",
    "    def step(self, action:int):\n",
    "        # the character health is 28 when fully healed, 27 when shield pops, and 26 when dead\n",
    "        life = pm.read_int(self.getPointerAddress(base=healthModule, offsets=[0x490, 0x10, 0xE0, 0x0, 0xB8, 0xF0, 0xE10]))\n",
    "\n",
    "        obs = self.get_observation()\n",
    "        \n",
    "        if life <= 26:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        pydirectinput.press(self.actionsDict[action])\n",
    "        \n",
    "        reward = self.reward_fn(life)\n",
    "\n",
    "        info = {}\n",
    "        return obs, reward, done, info\n",
    "    \n",
    "    def reward_fn(self, life):\n",
    "        gameScore = pm.read_int(self.getPointerAddress(base=scoreModule, offsets=[0x10, 0x108, 0x0, 0xD0, 0x8, 0x60, 0xC8]))\n",
    "\n",
    "        # Calculate score reward\n",
    "        scoreDelta = gameScore - self.previousScore\n",
    "        self.previousScore = gameScore\n",
    "\n",
    "        # Calculate health reward\n",
    "        healthDelta = life - self.previousHealth\n",
    "        self.previousHealth = life\n",
    "\n",
    "        # Calculate overall reward\n",
    "        score = (scoreDelta / 100) + (healthDelta * 100.5) + 0.5\n",
    "        return score\n",
    "    \n",
    "    # function to read memory from game\n",
    "    def getPointerAddress(self, base, offsets:list):\n",
    "        remote_pointer = RemotePointer(pm.process_handle, base)\n",
    "        for offset in offsets:\n",
    "            if offset != offsets[-1]:\n",
    "                remote_pointer = RemotePointer(pm.process_handle, remote_pointer.value + offset)\n",
    "            else:\n",
    "                return remote_pointer.value + offset\n",
    "\n",
    "    def reset(self):\n",
    "        time.sleep(0.9)\n",
    "        pydirectinput.press(\"enter\")\n",
    "        time.sleep(1.3)\n",
    "        obs = self.get_observation()\n",
    "        # reset variables\n",
    "        self.previousScore = 0\n",
    "        self.previousHealth = 28\n",
    "        return obs\n",
    "    \n",
    "    def get_observation(self):\n",
    "        obs = np.array(mss.mss().grab(self.monitor), dtype=np.uint8)\n",
    "        small_img = cv2.resize(obs, (80, 80))\n",
    "        img_gray = cv2.cvtColor(small_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        np_img = np.expand_dims(img_gray, axis=0)\n",
    "        return np_img\n",
    "\n",
    "    # Visualize the game\n",
    "    def render(self):\n",
    "        cv2.imshow(\"GAME\", self.get_observation()[0, :, :])\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            self.close\n",
    "\n",
    "    # This closes down the observation\n",
    "    def close(self):\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"C:/Users/imnot/Source/Repos/2023/Python/RL/BitBlaster/Saved_Models/\"\n",
    "LOG_DIR = \"C:/Users/imnot/runs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BitBlaster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code, might delete\n",
    "#env = Monitor(env, LOG_DIR)\n",
    "#env = DummyVecEnv([lambda:env])\n",
    "#env = VecFrameStack(env, 4, channels_order=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"CnnPolicy\", env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep so that i can click on the game window before it learns\n",
    "time.sleep(2)\n",
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sleep so that i can click on the game window before it learns\n",
    "time.sleep(2)\n",
    "# open the render window\n",
    "env.render()\n",
    "cv2.moveWindow('GAME',300,700)\n",
    "for episode in range(5):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        # here, we can see how fast the agent can see the game\n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(env.action_space.sample())\n",
    "        total_reward += reward\n",
    "    print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
